cp -rf py3.12/examples .
cp -rf py3.12/doc .
cp -f py3.12/index.html .
cp -rf py3.12/_multiprocess _multiprocess
cp -rf Python-3.13.0a1/Modules/_multiprocessing Modules/_multiprocess
cp -rf py3.12/multiprocess multiprocess
# ----------------------------------------------------------------------
diff Python-3.13.0a1/Modules/_multiprocessing/semaphore.c Modules/_multiprocess/semaphore.c
10c10
< #include "multiprocessing.h"
---
> #include "multiprocess.h"
39,40c39,40
< module _multiprocessing
< class _multiprocessing.SemLock "SemLockObject *" "&_PyMp_SemLockType"
---
> module _multiprocess
> class _multiprocess.SemLock "SemLockObject *" "&_PyMp_SemLockType"
84c84
< _multiprocessing.SemLock.acquire
---
> _multiprocess.SemLock.acquire
93c93
< _multiprocessing_SemLock_acquire_impl(SemLockObject *self, int blocking,
---
> _multiprocess_SemLock_acquire_impl(SemLockObject *self, int blocking,
175c175
< _multiprocessing.SemLock.release
---
> _multiprocess.SemLock.release
181c181
< _multiprocessing_SemLock_release_impl(SemLockObject *self)
---
> _multiprocess_SemLock_release_impl(SemLockObject *self)
300c300
< _multiprocessing.SemLock.acquire
---
> _multiprocess.SemLock.acquire
309c309
< _multiprocessing_SemLock_acquire_impl(SemLockObject *self, int blocking,
---
> _multiprocess_SemLock_acquire_impl(SemLockObject *self, int blocking,
385c385
< _multiprocessing.SemLock.release
---
> _multiprocess.SemLock.release
391c391
< _multiprocessing_SemLock_release_impl(SemLockObject *self)
---
> _multiprocess_SemLock_release_impl(SemLockObject *self)
475c475
< _multiprocessing.SemLock.__new__
---
> _multiprocess.SemLock.__new__
486c486
< _multiprocessing_SemLock_impl(PyTypeObject *type, int kind, int value,
---
> _multiprocess_SemLock_impl(PyTypeObject *type, int kind, int value,
534c534
< _multiprocessing.SemLock._rebuild
---
> _multiprocess.SemLock._rebuild
545c545
< _multiprocessing_SemLock__rebuild_impl(PyTypeObject *type, SEM_HANDLE handle,
---
> _multiprocess_SemLock__rebuild_impl(PyTypeObject *type, SEM_HANDLE handle,
586c586
< _multiprocessing.SemLock._count
---
> _multiprocess.SemLock._count
592c592
< _multiprocessing_SemLock__count_impl(SemLockObject *self)
---
> _multiprocess_SemLock__count_impl(SemLockObject *self)
599c599
< _multiprocessing.SemLock._is_mine
---
> _multiprocess.SemLock._is_mine
605c605
< _multiprocessing_SemLock__is_mine_impl(SemLockObject *self)
---
> _multiprocess_SemLock__is_mine_impl(SemLockObject *self)
613c613
< _multiprocessing.SemLock._get_value
---
> _multiprocess.SemLock._get_value
619c619
< _multiprocessing_SemLock__get_value_impl(SemLockObject *self)
---
> _multiprocess_SemLock__get_value_impl(SemLockObject *self)
638c638
< _multiprocessing.SemLock._is_zero
---
> _multiprocess.SemLock._is_zero
644c644
< _multiprocessing_SemLock__is_zero_impl(SemLockObject *self)
---
> _multiprocess_SemLock__is_zero_impl(SemLockObject *self)
666c666
< _multiprocessing.SemLock._after_fork
---
> _multiprocess.SemLock._after_fork
672c672
< _multiprocessing_SemLock__after_fork_impl(SemLockObject *self)
---
> _multiprocess_SemLock__after_fork_impl(SemLockObject *self)
680c680
< _multiprocessing.SemLock.__enter__
---
> _multiprocess.SemLock.__enter__
686c686
< _multiprocessing_SemLock___enter___impl(SemLockObject *self)
---
> _multiprocess_SemLock___enter___impl(SemLockObject *self)
689c689
<     return _multiprocessing_SemLock_acquire_impl(self, 1, Py_None);
---
>     return _multiprocess_SemLock_acquire_impl(self, 1, Py_None);
693c693
< _multiprocessing.SemLock.__exit__
---
> _multiprocess.SemLock.__exit__
704c704
< _multiprocessing_SemLock___exit___impl(SemLockObject *self,
---
> _multiprocess_SemLock___exit___impl(SemLockObject *self,
709c709
<     return _multiprocessing_SemLock_release_impl(self);
---
>     return _multiprocess_SemLock_release_impl(self);
724,733c724,733
<     _MULTIPROCESSING_SEMLOCK_ACQUIRE_METHODDEF
<     _MULTIPROCESSING_SEMLOCK_RELEASE_METHODDEF
<     _MULTIPROCESSING_SEMLOCK___ENTER___METHODDEF
<     _MULTIPROCESSING_SEMLOCK___EXIT___METHODDEF
<     _MULTIPROCESSING_SEMLOCK__COUNT_METHODDEF
<     _MULTIPROCESSING_SEMLOCK__IS_MINE_METHODDEF
<     _MULTIPROCESSING_SEMLOCK__GET_VALUE_METHODDEF
<     _MULTIPROCESSING_SEMLOCK__IS_ZERO_METHODDEF
<     _MULTIPROCESSING_SEMLOCK__REBUILD_METHODDEF
<     _MULTIPROCESSING_SEMLOCK__AFTER_FORK_METHODDEF
---
>     _MULTIPROCESS_SEMLOCK_ACQUIRE_METHODDEF
>     _MULTIPROCESS_SEMLOCK_RELEASE_METHODDEF
>     _MULTIPROCESS_SEMLOCK___ENTER___METHODDEF
>     _MULTIPROCESS_SEMLOCK___EXIT___METHODDEF
>     _MULTIPROCESS_SEMLOCK__COUNT_METHODDEF
>     _MULTIPROCESS_SEMLOCK__IS_MINE_METHODDEF
>     _MULTIPROCESS_SEMLOCK__GET_VALUE_METHODDEF
>     _MULTIPROCESS_SEMLOCK__IS_ZERO_METHODDEF
>     _MULTIPROCESS_SEMLOCK__REBUILD_METHODDEF
>     _MULTIPROCESS_SEMLOCK__AFTER_FORK_METHODDEF
764c764
<     {Py_tp_new, _multiprocessing_SemLock},
---
>     {Py_tp_new, _multiprocess_SemLock},
772c772
<     .name = "_multiprocessing.SemLock",
---
>     .name = "_multiprocess.SemLock",
diff Python-3.13.0a1/Modules/_multiprocessing/multiprocessing.h Modules/_multiprocess/multiprocess.h 
1,2c1,2
< #ifndef MULTIPROCESSING_H
< #define MULTIPROCESSING_H
---
> #ifndef MULTIPROCESS_H
> #define MULTIPROCESS_H
104c104
< #endif /* MULTIPROCESSING_H */
---
> #endif /* MULTIPROCESS_H */
diff Python-3.13.0a1/Modules/_multiprocessing/multiprocessing.c Modules/_multiprocess/multiprocess.c
2c2
<  * Extension module used by multiprocessing package
---
>  * Extension module used by multiprocess package
4c4
<  * multiprocessing.c
---
>  * multiprocess.c
10c10
< #include "multiprocessing.h"
---
> #include "multiprocess.h"
30c30
< module _multiprocessing
---
> module _multiprocess
77c77
< _multiprocessing.closesocket
---
> _multiprocess.closesocket
85c85
< _multiprocessing_closesocket_impl(PyObject *module, HANDLE handle)
---
> _multiprocess_closesocket_impl(PyObject *module, HANDLE handle)
100c100
< _multiprocessing.recv
---
> _multiprocess.recv
109c109
< _multiprocessing_recv_impl(PyObject *module, HANDLE handle, int size)
---
> _multiprocess_recv_impl(PyObject *module, HANDLE handle, int size)
132c132
< _multiprocessing.send
---
> _multiprocess.send
141c141
< _multiprocessing_send_impl(PyObject *module, HANDLE handle, Py_buffer *buf)
---
> _multiprocess_send_impl(PyObject *module, HANDLE handle, Py_buffer *buf)
160c160
< _multiprocessing.sem_unlink
---
> _multiprocess.sem_unlink
168c168
< _multiprocessing_sem_unlink_impl(PyObject *module, const char *name)
---
> _multiprocess_sem_unlink_impl(PyObject *module, const char *name)
196c196
< multiprocessing_exec(PyObject *module)
---
> multiprocess_exec(PyObject *module)
277,278c277,278
< static PyModuleDef_Slot multiprocessing_slots[] = {
<     {Py_mod_exec, multiprocessing_exec},
---
> static PyModuleDef_Slot multiprocess_slots[] = {
>     {Py_mod_exec, multiprocess_exec},
283c283
< static struct PyModuleDef multiprocessing_module = {
---
> static struct PyModuleDef multiprocess_module = {
285c285
<     .m_name = "_multiprocessing",
---
>     .m_name = "_multiprocess",
288c288
<     .m_slots = multiprocessing_slots,
---
>     .m_slots = multiprocess_slots,
292c292
< PyInit__multiprocessing(void)
---
> PyInit__multiprocess(void)
294c294
<     return PyModuleDef_Init(&multiprocessing_module);
---
>     return PyModuleDef_Init(&multiprocess_module);
# ----------------------------------------------------------------------
diff Python-3.12.0rc3/Lib/multiprocessing/connection.py Python-3.13.0a1/Lib/multiprocessing/connection.py
11a12
> import errno
273a275
>         _send_ov = None
275a278,281
>             ov = self._send_ov
>             if ov is not None:
>                 # Interrupt WaitForMultipleObjects() in _send_bytes()
>                 ov.cancel()
278a285,288
>             if self._send_ov is not None:
>                 # A connection should only be used by a single thread
>                 raise ValueError("concurrent send_bytes() calls "
>                                  "are not supported")
279a290
>             self._send_ov = ov
288a300
>                 self._send_ov = None
289a302,306
>             if err == _winapi.ERROR_OPERATION_ABORTED:
>                 # close() was called by another thread while
>                 # WaitForMultipleObjects() was waiting for the overlapped
>                 # operation.
>                 raise OSError(errno.EPIPE, "handle is closed")
diff Python-3.12.0rc3/Lib/multiprocessing/managers.py Python-3.13.0a1/Lib/multiprocessing/managers.py
93c93,96
<     raise convert_to_error(kind, result)
---
>     try:
>         raise convert_to_error(kind, result)
>     finally:
>         del result  # break reference cycle
836c839,842
<         raise convert_to_error(kind, result)
---
>         try:
>             raise convert_to_error(kind, result)
>         finally:
>             del result   # break reference cycle
diff Python-3.12.0rc3/Lib/multiprocessing/pool.py Python-3.13.0a1/Lib/multiprocessing/pool.py
203c203
<             processes = os.cpu_count() or 1
---
>             processes = os.process_cpu_count() or 1
diff Python-3.12.0rc3/Lib/multiprocessing/popen_spawn_win32.py Python-3.13.0a1/Lib/multiprocessing/popen_spawn_win32.py
16a17
> # Exit code used by Popen.terminate()
125,126c126,130
<             except OSError:
<                 if self.wait(timeout=1.0) is None:
---
>             except PermissionError:
>                 # ERROR_ACCESS_DENIED (winerror 5) is received when the
>                 # process already died.
>                 code = _winapi.GetExitCodeProcess(int(self._handle))
>                 if code == _winapi.STILL_ACTIVE:
127a132,134
>                 self.returncode = code
>             else:
>                 self.returncode = -signal.SIGTERM
diff Python-3.12.0rc3/Lib/multiprocessing/queues.py Python-3.13.0a1/Lib/multiprocessing/queues.py
160a161,174
>     def _terminate_broken(self):
>         # Close a Queue on error.
> 
>         # gh-94777: Prevent queue writing to a pipe which is no longer read.
>         self._reader.close()
> 
>         # gh-107219: Close the connection writer which can unblock
>         # Queue._feed() if it was stuck in send_bytes().
>         if sys.platform == 'win32':
>             self._writer.close()
> 
>         self.close()
>         self.join_thread()
> 
172c186,187
<             name='QueueFeederThread'
---
>             name='QueueFeederThread',
>             daemon=True,
174d188
<         self._thread.daemon = True
176,178c190,198
<         debug('doing self._thread.start()')
<         self._thread.start()
<         debug('... done self._thread.start()')
---
>         try:
>             debug('doing self._thread.start()')
>             self._thread.start()
>             debug('... done self._thread.start()')
>         except:
>             # gh-109047: During Python finalization, creating a thread
>             # can fail with RuntimeError.
>             self._thread = None
>             raise
diff Python-3.12.0rc3/Lib/multiprocessing/resource_tracker.py Python-3.13.0a1/Lib/multiprocessing/resource_tracker.py
53a54,57
> class ReentrantCallError(RuntimeError):
>     pass
> 
> 
57c61
<         self._lock = threading.Lock()
---
>         self._lock = threading.RLock()
60a65,72
>     def _reentrant_call_error(self):
>         # gh-109629: this happens if an explicit call to the ResourceTracker
>         # gets interrupted by a garbage collection, invoking a finalizer (*)
>         # that itself calls back into ResourceTracker.
>         #   (*) for example the SemLock finalizer
>         raise ReentrantCallError(
>             "Reentrant call into the multiprocessing resource tracker")
> 
62a75,78
>             # This should not happen (_stop() isn't called by a finalizer)
>             # but we check for it anyway.
>             if self._lock._recursion_count() > 1:
>                 return self._reentrant_call_error()
83a100,102
>             if self._lock._recursion_count() > 1:
>                 # The code below is certainly not reentrant-safe, so bail out
>                 return self._reentrant_call_error()
162c181,191
<         self.ensure_running()
---
>         try:
>             self.ensure_running()
>         except ReentrantCallError:
>             # The code below might or might not work, depending on whether
>             # the resource tracker was already running and still alive.
>             # Better warn the user.
>             # (XXX is warnings.warn itself reentrant-safe? :-)
>             warnings.warn(
>                 f"ResourceTracker called reentrantly for resource cleanup, "
>                 f"which is unsupported. "
>                 f"The {rtype} object {name!r} might leak.")
178a208
> 
224,226c254,257
<                     warnings.warn('resource_tracker: There appear to be %d '
<                                   'leaked %s objects to clean up at shutdown' %
<                                   (len(rtype_cache), rtype))
---
>                     warnings.warn(
>                         f'resource_tracker: There appear to be {len(rtype_cache)} '
>                         f'leaked {rtype} objects to clean up at shutdown: {rtype_cache}'
>                     )
diff Python-3.12.0rc3/Lib/multiprocessing/util.py Python-3.13.0a1/Lib/multiprocessing/util.py
67,68c67
<     logging._acquireLock()
<     try:
---
>     with logging._lock:
82,84d80
<     finally:
<         logging._releaseLock()
< 
# ----------------------------------------------------------------------
diff Python-3.12.0rc3/Lib/test/_test_multiprocessing.py Python-3.13.0a1/Lib/test/_test_multiprocessing.py 
53c53
< from multiprocessing.connection import wait, AuthenticationError
---
> from multiprocessing.connection import wait
81,82c81,82
< if support.check_sanitizer(address=True):
<     # bpo-45200: Skip multiprocessing tests if Python is built with ASAN to
---
> if support.HAVE_ASAN_FORK_BUG:
>     # gh-89363: Skip multiprocessing tests if Python is built with ASAN to
84c84,89
<     raise unittest.SkipTest("libasan has a pthread_create() dead lock")
---
>     raise unittest.SkipTest("libasan has a pthread_create() dead lock related to thread+fork")
> 
> 
> # gh-110666: Tolerate a difference of 100 ms when comparing timings
> # (clock resolution)
> CLOCK_RES = 0.100
560,561c565
<         if os.name != 'nt':
<             self.assertEqual(exitcode, -signal.SIGTERM)
---
>         self.assertEqual(exitcode, -signal.SIGTERM)
566a571,572
>         else:
>             self.assertEqual(exitcode, -signal.SIGTERM)
1653c1659
<             expected = 0.1
---
>             expected = 0.100
1657,1658c1663
<             # borrow logic in assertTimeout() from test/lock_tests.py
<             if not result and expected * 0.6 < dt < expected * 10.0:
---
>             if not result and (expected - CLOCK_RES) <= dt:
1677c1682
<             time.sleep(0.01)
---
>             time.sleep(0.010)
2576c2581
<         res = self.pool.apply_async(sqr, (6, TIMEOUT2 + 1.0))
---
>         res = self.pool.apply_async(sqr, (6, TIMEOUT2 + support.SHORT_TIMEOUT))
2680,2682c2685,2687
<         result = self.pool.map_async(
<             time.sleep, [0.1 for i in range(10000)], chunksize=1
<             )
---
>         # Simulate slow tasks which take "forever" to complete
>         args = [support.LONG_TIMEOUT for i in range(10_000)]
>         result = self.pool.map_async(time.sleep, args, chunksize=1)
2684,2687c2689
<         join = TimingWrapper(self.pool.join)
<         join()
<         # Sanity check the pool didn't wait for all tasks to finish
<         self.assertLess(join.elapsed, 2.0)
---
>         self.pool.join()
3153a3156,3193
> 
> class FakeConnection:
>     def send(self, payload):
>         pass
> 
>     def recv(self):
>         return '#ERROR', pyqueue.Empty()
> 
> class TestManagerExceptions(unittest.TestCase):
>     # Issue 106558: Manager exceptions avoids creating cyclic references.
>     def setUp(self):
>         self.mgr = multiprocessing.Manager()
> 
>     def tearDown(self):
>         self.mgr.shutdown()
>         self.mgr.join()
> 
>     def test_queue_get(self):
>         queue = self.mgr.Queue()
>         if gc.isenabled():
>             gc.disable()
>             self.addCleanup(gc.enable)
>         try:
>             queue.get_nowait()
>         except pyqueue.Empty as e:
>             wr = weakref.ref(e)
>         self.assertEqual(wr(), None)
> 
>     def test_dispatch(self):
>         if gc.isenabled():
>             gc.disable()
>             self.addCleanup(gc.enable)
>         try:
>             multiprocessing.managers.dispatch(FakeConnection(), None, None)
>         except pyqueue.Empty as e:
>             wr = weakref.ref(e)
>         self.assertEqual(wr(), None)
> 
4872c4912
<                 time.sleep(random.random()*0.1)
---
>                 time.sleep(random.random() * 0.100)
4912c4952
<                 time.sleep(random.random()*0.1)
---
>                 time.sleep(random.random() * 0.100)
4961c5001
<         expected = 5
---
>         timeout = 5.0  # seconds
4965c5005
<         res = wait([a, b], expected)
---
>         res = wait([a, b], timeout)
4969,4970c5009
<         self.assertLess(delta, expected * 2)
<         self.assertGreater(delta, expected * 0.5)
---
>         self.assertGreater(delta, timeout - CLOCK_RES)
4973,4974d5011
< 
<         start = time.monotonic()
4976,4977d5012
<         delta = time.monotonic() - start
< 
4979d5013
<         self.assertLess(delta, 0.4)
5437c5471,5473
<         self.assertEqual(results, [2, 1])
---
>         # gh-109706: queue.put(1) can write into the queue before queue.put(2),
>         # there is no synchronization in the test.
>         self.assertSetEqual(set(results), set([2, 1]))
# ----------------------------------------------------------------------
diff Python-3.13.0a1/Modules/_multiprocessing/clinic/posixshmem.c.h Python-3.13.0a2/Modules/_multiprocessing/clinic/posixshmem.c.h
5,9d4
< #if defined(Py_BUILD_CORE) && !defined(Py_BUILD_CORE_MODULE)
< #  include "pycore_gc.h"          // PyGC_Head
< #  include "pycore_runtime.h"     // _Py_ID()
< #endif
< 
19c14
<     {"shm_open", _PyCFunction_CAST(_posixshmem_shm_open), METH_FASTCALL|METH_KEYWORDS, _posixshmem_shm_open__doc__},
---
>     {"shm_open", (PyCFunction)(void(*)(void))_posixshmem_shm_open, METH_VARARGS|METH_KEYWORDS, _posixshmem_shm_open__doc__},
26c21
< _posixshmem_shm_open(PyObject *module, PyObject *const *args, Py_ssize_t nargs, PyObject *kwnames)
---
> _posixshmem_shm_open(PyObject *module, PyObject *args, PyObject *kwargs)
29,55c24
<     #if defined(Py_BUILD_CORE) && !defined(Py_BUILD_CORE_MODULE)
< 
<     #define NUM_KEYWORDS 3
<     static struct {
<         PyGC_Head _this_is_not_used;
<         PyObject_VAR_HEAD
<         PyObject *ob_item[NUM_KEYWORDS];
<     } _kwtuple = {
<         .ob_base = PyVarObject_HEAD_INIT(&PyTuple_Type, NUM_KEYWORDS)
<         .ob_item = { &_Py_ID(path), &_Py_ID(flags), &_Py_ID(mode), },
<     };
<     #undef NUM_KEYWORDS
<     #define KWTUPLE (&_kwtuple.ob_base.ob_base)
< 
<     #else  // !Py_BUILD_CORE
<     #  define KWTUPLE NULL
<     #endif  // !Py_BUILD_CORE
< 
<     static const char * const _keywords[] = {"path", "flags", "mode", NULL};
<     static _PyArg_Parser _parser = {
<         .keywords = _keywords,
<         .fname = "shm_open",
<         .kwtuple = KWTUPLE,
<     };
<     #undef KWTUPLE
<     PyObject *argsbuf[3];
<     Py_ssize_t noptargs = nargs + (kwnames ? PyTuple_GET_SIZE(kwnames) : 0) - 2;
---
>     static char *_keywords[] = {"path", "flags", "mode", NULL};
61,78c30,31
<     args = _PyArg_UnpackKeywords(args, nargs, NULL, kwnames, &_parser, 2, 3, 0, argsbuf);
<     if (!args) {
<         goto exit;
<     }
<     if (!PyUnicode_Check(args[0])) {
<         _PyArg_BadArgument("shm_open", "argument 'path'", "str", args[0]);
<         goto exit;
<     }
<     path = args[0];
<     flags = PyLong_AsInt(args[1]);
<     if (flags == -1 && PyErr_Occurred()) {
<         goto exit;
<     }
<     if (!noptargs) {
<         goto skip_optional_pos;
<     }
<     mode = PyLong_AsInt(args[2]);
<     if (mode == -1 && PyErr_Occurred()) {
---
>     if (!PyArg_ParseTupleAndKeywords(args, kwargs, "Ui|i:shm_open", _keywords,
>         &path, &flags, &mode))
80,81d32
<     }
< skip_optional_pos:
107c58
<     {"shm_unlink", _PyCFunction_CAST(_posixshmem_shm_unlink), METH_FASTCALL|METH_KEYWORDS, _posixshmem_shm_unlink__doc__},
---
>     {"shm_unlink", (PyCFunction)(void(*)(void))_posixshmem_shm_unlink, METH_VARARGS|METH_KEYWORDS, _posixshmem_shm_unlink__doc__},
113c64
< _posixshmem_shm_unlink(PyObject *module, PyObject *const *args, Py_ssize_t nargs, PyObject *kwnames)
---
> _posixshmem_shm_unlink(PyObject *module, PyObject *args, PyObject *kwargs)
116,141c67
<     #if defined(Py_BUILD_CORE) && !defined(Py_BUILD_CORE_MODULE)
< 
<     #define NUM_KEYWORDS 1
<     static struct {
<         PyGC_Head _this_is_not_used;
<         PyObject_VAR_HEAD
<         PyObject *ob_item[NUM_KEYWORDS];
<     } _kwtuple = {
<         .ob_base = PyVarObject_HEAD_INIT(&PyTuple_Type, NUM_KEYWORDS)
<         .ob_item = { &_Py_ID(path), },
<     };
<     #undef NUM_KEYWORDS
<     #define KWTUPLE (&_kwtuple.ob_base.ob_base)
< 
<     #else  // !Py_BUILD_CORE
<     #  define KWTUPLE NULL
<     #endif  // !Py_BUILD_CORE
< 
<     static const char * const _keywords[] = {"path", NULL};
<     static _PyArg_Parser _parser = {
<         .keywords = _keywords,
<         .fname = "shm_unlink",
<         .kwtuple = KWTUPLE,
<     };
<     #undef KWTUPLE
<     PyObject *argsbuf[1];
---
>     static char *_keywords[] = {"path", NULL};
144,145c70,71
<     args = _PyArg_UnpackKeywords(args, nargs, NULL, kwnames, &_parser, 1, 1, 0, argsbuf);
<     if (!args) {
---
>     if (!PyArg_ParseTupleAndKeywords(args, kwargs, "U:shm_unlink", _keywords,
>         &path))
147,152d72
<     }
<     if (!PyUnicode_Check(args[0])) {
<         _PyArg_BadArgument("shm_unlink", "argument 'path'", "str", args[0]);
<         goto exit;
<     }
<     path = args[0];
168c88
< /*[clinic end generated code: output=2f356903a281d857 input=a9049054013a1b77]*/
---
> /*[clinic end generated code: output=be0661dbed83ea23 input=a9049054013a1b77]*/
diff Python-3.13.0a1/Modules/_multiprocessing/clinic/semaphore.c.h Python-3.13.0a2/Modules/_multiprocessing/clinic/semaphore.c.h
8a9
> #include "pycore_modsupport.h"    // _PyArg_UnpackKeywords()
544c545
< /*[clinic end generated code: output=e8ea65f8cba8e173 input=a9049054013a1b77]*/
---
> /*[clinic end generated code: output=d57992037e6770b6 input=a9049054013a1b77]*/
diff Python-3.13.0a1/Modules/_multiprocessing/clinic/multiprocessing.c.h Python-3.13.0a2/Modules/_multiprocessing/clinic/multiprocessing.c.h
4a5,6
> #include "pycore_modsupport.h"    // _PyArg_CheckPositional()
> 
105,108d106
<     if (!PyBuffer_IsContiguous(&buf, 'C')) {
<         _PyArg_BadArgument("send", "argument 2", "contiguous buffer", args[1]);
<         goto exit;
<     }
169c167
< /*[clinic end generated code: output=8b91c020d4353cc5 input=a9049054013a1b77]*/
---
> /*[clinic end generated code: output=73b4cb8428d816da input=a9049054013a1b77]*/
# ----------------------------------------------------------------------
diff Python-3.13.0a1/Modules/_multiprocessing/posixshmem.c Python-3.13.0a2/Modules/_multiprocessing/posixshmem.c
4a5,11
> #include "pyconfig.h"   // Py_GIL_DISABLED
> 
> #ifndef Py_GIL_DISABLED
> // Need limited C API version 3.12 for Py_MOD_PER_INTERPRETER_GIL_SUPPORTED
> #define Py_LIMITED_API 0x030c0000
> #endif
> 
7c14
< // for shm_open() and shm_unlink()
---
> #include <errno.h>                // EINTR
9c16
< #include <sys/mman.h>
---
> #  include <sys/mman.h>           // shm_open(), shm_unlink()
11a19
> 
43c51
<     const char *name = PyUnicode_AsUTF8(path);
---
>     const char *name = PyUnicode_AsUTF8AndSize(path, NULL);
82c90
<     const char *name = PyUnicode_AsUTF8(path);
---
>     const char *name = PyUnicode_AsUTF8AndSize(path, NULL);
# ----------------------------------------------------------------------
diff Python-3.13.0a1/Lib/multiprocessing/managers.py Python-3.13.0a2/Lib/multiprocessing/managers.py
1167a1168
>     __class_getitem__ = classmethod(types.GenericAlias)
1169c1170,1171
< DictProxy = MakeProxyType('DictProxy', (
---
> 
> _BaseDictProxy = MakeProxyType('DictProxy', (
1174c1176
< DictProxy._method_to_typeid_ = {
---
> _BaseDictProxy._method_to_typeid_ = {
1176a1179,1180
> class DictProxy(_BaseDictProxy):
>     __class_getitem__ = classmethod(types.GenericAlias)
# ----------------------------------------------------------------------
diff Python-3.13.0a1/Lib/test/_test_multiprocessing.py Python-3.13.0a2/Lib/test/_test_multiprocessing.py 
2441,2442c2441,2445
< def sqr(x, wait=0.0):
<     time.sleep(wait)
---
> def sqr(x, wait=0.0, event=None):
>     if event is None:
>         time.sleep(wait)
>     else:
>         event.wait(wait)
2581,2584c2584,2595
<         res = self.pool.apply_async(sqr, (6, TIMEOUT2 + support.SHORT_TIMEOUT))
<         get = TimingWrapper(res.get)
<         self.assertRaises(multiprocessing.TimeoutError, get, timeout=TIMEOUT2)
<         self.assertTimingAlmostEqual(get.elapsed, TIMEOUT2)
---
>         p = self.Pool(3)
>         try:
>             event = threading.Event() if self.TYPE == 'threads' else None
>             res = p.apply_async(sqr, (6, TIMEOUT2 + support.SHORT_TIMEOUT, event))
>             get = TimingWrapper(res.get)
>             self.assertRaises(multiprocessing.TimeoutError, get, timeout=TIMEOUT2)
>             self.assertTimingAlmostEqual(get.elapsed, TIMEOUT2)
>         finally:
>             if event is not None:
>                 event.set()
>             p.terminate()
>             p.join()
2684a2696,2698
>         if self.TYPE == 'threads':
>             self.skipTest("Threads cannot be terminated")
> 
2685a2700
>         p = self.Pool(3)
2687,2689c2702,2704
<         result = self.pool.map_async(time.sleep, args, chunksize=1)
<         self.pool.terminate()
<         self.pool.join()
---
>         result = p.map_async(time.sleep, args, chunksize=1)
>         p.terminate()
>         p.join()
# ----------------------------------------------------------------------
diff Python-3.13.0a2/Lib/multiprocessing/managers.py Python-3.13.0a3/Lib/multiprocessing/managers.py
159c159
<         self.listener = Listener(address=address, backlog=16)
---
>         self.listener = Listener(address=address, backlog=128)
diff Python-3.13.0a2/Lib/multiprocessing/popen_spawn_win32.py Python-3.13.0a3/Lib/multiprocessing/popen_spawn_win32.py
104,115c104,117
<         if self.returncode is None:
<             if timeout is None:
<                 msecs = _winapi.INFINITE
<             else:
<                 msecs = max(0, int(timeout * 1000 + 0.5))
< 
<             res = _winapi.WaitForSingleObject(int(self._handle), msecs)
<             if res == _winapi.WAIT_OBJECT_0:
<                 code = _winapi.GetExitCodeProcess(self._handle)
<                 if code == TERMINATE:
<                     code = -signal.SIGTERM
<                 self.returncode = code
---
>         if self.returncode is not None:
>             return self.returncode
> 
>         if timeout is None:
>             msecs = _winapi.INFINITE
>         else:
>             msecs = max(0, int(timeout * 1000 + 0.5))
> 
>         res = _winapi.WaitForSingleObject(int(self._handle), msecs)
>         if res == _winapi.WAIT_OBJECT_0:
>             code = _winapi.GetExitCodeProcess(self._handle)
>             if code == TERMINATE:
>                 code = -signal.SIGTERM
>             self.returncode = code
123,134c125,140
<         if self.returncode is None:
<             try:
<                 _winapi.TerminateProcess(int(self._handle), TERMINATE)
<             except PermissionError:
<                 # ERROR_ACCESS_DENIED (winerror 5) is received when the
<                 # process already died.
<                 code = _winapi.GetExitCodeProcess(int(self._handle))
<                 if code == _winapi.STILL_ACTIVE:
<                     raise
<                 self.returncode = code
<             else:
<                 self.returncode = -signal.SIGTERM
---
>         if self.returncode is not None:
>             return
> 
>         try:
>             _winapi.TerminateProcess(int(self._handle), TERMINATE)
>         except PermissionError:
>             # ERROR_ACCESS_DENIED (winerror 5) is received when the
>             # process already died.
>             code = _winapi.GetExitCodeProcess(int(self._handle))
>             if code == _winapi.STILL_ACTIVE:
>                 raise
> 
>         # gh-113009: Don't set self.returncode. Even if GetExitCodeProcess()
>         # returns an exit code different than STILL_ACTIVE, the process can
>         # still be running. Only set self.returncode once WaitForSingleObject()
>         # returns WAIT_OBJECT_0 in wait().
diff Python-3.13.0a2/Lib/multiprocessing/resource_sharer.py Python-3.13.0a3/Lib/multiprocessing/resource_sharer.py
126c126
<         self._listener = Listener(authkey=process.current_process().authkey)
---
>         self._listener = Listener(authkey=process.current_process().authkey, backlog=128)
diff Python-3.13.0a2/Lib/multiprocessing/shared_memory.py Python-3.13.0a3/Lib/multiprocessing/shared_memory.py
73a74
>     _track = True
75c76
<     def __init__(self, name=None, create=False, size=0):
---
>     def __init__(self, name=None, create=False, size=0, *, track=True):
84a86
>         self._track = track
119,120c121,122
< 
<             resource_tracker.register(self._name, "shared_memory")
---
>             if self._track:
>                 resource_tracker.register(self._name, "shared_memory")
239,241c241,250
<         In order to ensure proper cleanup of resources, unlink should be
<         called once (and only once) across all processes which have access
<         to the shared memory block."""
---
>         Unlink should be called once (and only once) across all handles
>         which have access to the shared memory block, even if these
>         handles belong to different processes. Closing and unlinking may
>         happen in any order, but trying to access data inside a shared
>         memory block after unlinking may result in memory errors,
>         depending on platform.
> 
>         This method has no effect on Windows, where the only way to
>         delete a shared memory block is to close all handles."""
> 
244c253,254
<             resource_tracker.unregister(self._name, "shared_memory")
---
>             if self._track:
>                 resource_tracker.unregister(self._name, "shared_memory")
diff Python-3.13.0a2/Lib/multiprocessing/util.py Python-3.13.0a3/Lib/multiprocessing/util.py
46c46
<         _logger.log(SUBDEBUG, msg, *args)
---
>         _logger.log(SUBDEBUG, msg, *args, stacklevel=2)
50c50
<         _logger.log(DEBUG, msg, *args)
---
>         _logger.log(DEBUG, msg, *args, stacklevel=2)
54c54
<         _logger.log(INFO, msg, *args)
---
>         _logger.log(INFO, msg, *args, stacklevel=2)
58c58
<         _logger.log(SUBWARNING, msg, *args)
---
>         _logger.log(SUBWARNING, msg, *args, stacklevel=2)
# ----------------------------------------------------------------------
diff Python-3.13.0a2/Lib/test/_test_multiprocessing.py Python-3.13.0a3/Lib/test/_test_multiprocessing.py 
4457a4458,4510
>     @unittest.skipIf(os.name != "posix", "resource_tracker is posix only")
>     def test_shared_memory_untracking(self):
>         # gh-82300: When a separate Python process accesses shared memory
>         # with track=False, it must not cause the memory to be deleted
>         # when terminating.
>         cmd = '''if 1:
>             import sys
>             from multiprocessing.shared_memory import SharedMemory
>             mem = SharedMemory(create=False, name=sys.argv[1], track=False)
>             mem.close()
>         '''
>         mem = shared_memory.SharedMemory(create=True, size=10)
>         # The resource tracker shares pipes with the subprocess, and so
>         # err existing means that the tracker process has terminated now.
>         try:
>             rc, out, err = script_helper.assert_python_ok("-c", cmd, mem.name)
>             self.assertNotIn(b"resource_tracker", err)
>             self.assertEqual(rc, 0)
>             mem2 = shared_memory.SharedMemory(create=False, name=mem.name)
>             mem2.close()
>         finally:
>             try:
>                 mem.unlink()
>             except OSError:
>                 pass
>             mem.close()
> 
>     @unittest.skipIf(os.name != "posix", "resource_tracker is posix only")
>     def test_shared_memory_tracking(self):
>         # gh-82300: When a separate Python process accesses shared memory
>         # with track=True, it must cause the memory to be deleted when
>         # terminating.
>         cmd = '''if 1:
>             import sys
>             from multiprocessing.shared_memory import SharedMemory
>             mem = SharedMemory(create=False, name=sys.argv[1], track=True)
>             mem.close()
>         '''
>         mem = shared_memory.SharedMemory(create=True, size=10)
>         try:
>             rc, out, err = script_helper.assert_python_ok("-c", cmd, mem.name)
>             self.assertEqual(rc, 0)
>             self.assertIn(
>                 b"resource_tracker: There appear to be 1 leaked "
>                 b"shared_memory objects to clean up at shutdown", err)
>         finally:
>             try:
>                 mem.unlink()
>             except OSError:
>                 pass
>             resource_tracker.unregister(mem._name, "shared_memory")
>             mem.close()
> 
4673a4727,4749
>     def test_filename(self):
>         logger = multiprocessing.get_logger()
>         original_level = logger.level
>         try:
>             logger.setLevel(util.DEBUG)
>             stream = io.StringIO()
>             handler = logging.StreamHandler(stream)
>             logging_format = '[%(levelname)s] [%(filename)s] %(message)s'
>             handler.setFormatter(logging.Formatter(logging_format))
>             logger.addHandler(handler)
>             logger.info('1')
>             util.info('2')
>             logger.debug('3')
>             filename = os.path.basename(__file__)
>             log_record = stream.getvalue()
>             self.assertIn(f'[INFO] [{filename}] 1', log_record)
>             self.assertIn(f'[INFO] [{filename}] 2', log_record)
>             self.assertIn(f'[DEBUG] [{filename}] 3', log_record)
>         finally:
>             logger.setLevel(original_level)
>             logger.removeHandler(handler)
>             handler.close()
> 
# ----------------------------------------------------------------------
diff Python-3.13.0a3/Lib/multiprocessing/connection.py Python-3.13.0a4/Lib/multiprocessing/connection.py
22d21
< import _multiprocessing
30a30
>     import _multiprocessing
1013a1014,1024
>         # Windows limits WaitForMultipleObjects at 64 handles, and we use a
>         # few for synchronisation, so we switch to batched waits at 60.
>         if len(L) > 60:
>             try:
>                 res = _winapi.BatchedWaitForMultipleObjects(L, False, timeout)
>             except TimeoutError:
>                 return []
>             ready.extend(L[i] for i in res)
>             if res:
>                 L = [h for i, h in enumerate(L) if i > res[0] & i not in res]
>             timeout = 0
1015c1026,1027
<             res = _winapi.WaitForMultipleObjects(L, False, timeout)
---
>             short_L = L[:60] if len(L) > 60 else L
>             res = _winapi.WaitForMultipleObjects(short_L, False, timeout)
diff Python-3.13.0a3/Lib/test/_test_multiprocessing.py Python-3.13.0a4/Lib/test/_test_multiprocessing.py 
2695a2696,2698
>         # Simulate slow tasks which take "forever" to complete
>         sleep_time = support.LONG_TIMEOUT
> 
2697c2700,2703
<             self.skipTest("Threads cannot be terminated")
---
>             # Thread pool workers can't be forced to quit, so if the first
>             # task starts early enough, we will end up waiting for it.
>             # Sleep for a shorter time, so the test doesn't block.
>             sleep_time = 1
2699d2704
<         # Simulate slow tasks which take "forever" to complete
2701c2706
<         args = [support.LONG_TIMEOUT for i in range(10_000)]
---
>         args = [sleep_time for i in range(10_000)]
2702a2708
>         time.sleep(0.2)  # give some tasks a chance to start
6109a6116,6133
>     def test_large_pool(self):
>         #
>         # gh-89240: Check that large pools are always okay
>         #
>         testfn = os_helper.TESTFN
>         self.addCleanup(os_helper.unlink, testfn)
>         with open(testfn, 'w', encoding='utf-8') as f:
>             f.write(textwrap.dedent('''\
>                 import multiprocessing
>                 def f(x): return x*x
>                 if __name__ == '__main__':
>                     with multiprocessing.Pool(200) as p:
>                         print(sum(p.map(f, range(1000))))
>             '''))
>         rc, out, err = script_helper.assert_python_ok(testfn)
>         self.assertEqual("332833500", out.decode('utf-8').strip())
>         self.assertFalse(err, msg=err.decode('utf-8'))
> 
# ----------------------------------------------------------------------
diff Python-3.13.0a4/Modules/_multiprocessing/multiprocessing.c Python-3.13.0a5/Modules/_multiprocessing/multiprocessing.c
184c184
< #if !defined(POSIX_SEMAPHORES_NOT_ENABLED) && !defined(__ANDROID__)
---
> #if !defined(POSIX_SEMAPHORES_NOT_ENABLED)
diff Python-3.13.0a4/Modules/_multiprocessing/posixshmem.c Python-3.13.0a5/Modules/_multiprocessing/posixshmem.c
4a5
> // Need limited C API version 3.12 for Py_MOD_PER_INTERPRETER_GIL_SUPPORTED
6d6
< 
8,9c8
< // Need limited C API version 3.12 for Py_MOD_PER_INTERPRETER_GIL_SUPPORTED
< #define Py_LIMITED_API 0x030c0000
---
> #  define Py_LIMITED_API 0x030c0000
13a13
> #include <string.h>               // strlen()
51c51,52
<     const char *name = PyUnicode_AsUTF8AndSize(path, NULL);
---
>     Py_ssize_t name_size;
>     const char *name = PyUnicode_AsUTF8AndSize(path, &name_size);
54a56,59
>     if (strlen(name) != (size_t)name_size) {
>         PyErr_SetString(PyExc_ValueError, "embedded null character");
>         return -1;
>     }
90c95,96
<     const char *name = PyUnicode_AsUTF8AndSize(path, NULL);
---
>     Py_ssize_t name_size;
>     const char *name = PyUnicode_AsUTF8AndSize(path, &name_size);
93a100,103
>     if (strlen(name) != (size_t)name_size) {
>         PyErr_SetString(PyExc_ValueError, "embedded null character");
>         return NULL;
>     }
diff Python-3.13.0a4/Lib/multiprocessing/connection.py Python-3.13.0a5/Lib/multiprocessing/connection.py
478a479
> 
480c481
<         if self._authkey:
---
>         if self._authkey is not None:
diff Python-3.13.0a4/Lib/multiprocessing/queues.py Python-3.13.0a5/Lib/multiprocessing/queues.py
23,24d22
< import _multiprocessing
< 
diff Python-3.13.0a4/Lib/multiprocessing/resource_tracker.py Python-3.13.0a5/Lib/multiprocessing/resource_tracker.py
31a32,34
> def cleanup_noop(name):
>     raise RuntimeError('noop should never be registered or cleaned up')
> 
33c36,37
<     'noop': lambda: None,
---
>     'noop': cleanup_noop,
>     'dummy': lambda name: None,  # Dummy resource used in tests
63a68
>         self._exitcode = None
87c92,93
<             os.waitpid(self._pid, 0)
---
>             _, status = os.waitpid(self._pid, 0)
> 
89a96,101
>             try:
>                 self._exitcode = os.waitstatus_to_exitcode(status)
>             except ValueError:
>                 # os.waitstatus_to_exitcode may raise an exception for invalid values
>                 self._exitcode = None
> 
121a134
>                 self._exitcode = None
223a237,238
>     exit_code = 0
> 
244a260
>                     exit_code = 3
254,257c270,280
<                     warnings.warn(
<                         f'resource_tracker: There appear to be {len(rtype_cache)} '
<                         f'leaked {rtype} objects to clean up at shutdown: {rtype_cache}'
<                     )
---
>                     exit_code = 1
>                     if rtype == 'dummy':
>                         # The test 'dummy' resource is expected to leak.
>                         # We skip the warning (and *only* the warning) for it.
>                         pass
>                     else:
>                         warnings.warn(
>                             f'resource_tracker: There appear to be '
>                             f'{len(rtype_cache)} leaked {rtype} objects to '
>                             f'clean up at shutdown: {rtype_cache}'
>                         )
267a291
>                         exit_code = 2
270a295,296
> 
>         sys.exit(exit_code)
diff Python-3.13.0a4/Lib/multiprocessing/util.py Python-3.13.0a5/Lib/multiprocessing/util.py
105,109c105
<     if sys.platform == "linux":
<         return True
<     if hasattr(sys, 'getandroidapilevel'):
<         return True
<     return False
---
>     return sys.platform in ("linux", "android")
diff Python-3.13.0a4/Lib/test/_test_multiprocessing.py Python-3.13.0a5/Lib/test/_test_multiprocessing.py 
3506a3507,3530
>     def test_empty_authkey(self):
>         # bpo-43952: allow empty bytes as authkey
>         def handler(*args):
>             raise RuntimeError('Connection took too long...')
> 
>         def run(addr, authkey):
>             client = self.connection.Client(addr, authkey=authkey)
>             client.send(1729)
> 
>         key = b''
> 
>         with self.connection.Listener(authkey=key) as listener:
>             thread = threading.Thread(target=run, args=(listener.address, key))
>             thread.start()
>             try:
>                 with listener.accept() as d:
>                     self.assertEqual(d.recv(), 1729)
>             finally:
>                 thread.join()
> 
>         if self.TYPE == 'processes':
>             with self.assertRaises(OSError):
>                 listener.accept()
> 
3973a3998,4012
>     def test_shared_memory_name_with_embedded_null(self):
>         name_tsmb = self._new_shm_name('test01_null')
>         sms = shared_memory.SharedMemory(name_tsmb, create=True, size=512)
>         self.addCleanup(sms.unlink)
>         with self.assertRaises(ValueError):
>             shared_memory.SharedMemory(name_tsmb + '\0a', create=False, size=512)
>         if shared_memory._USE_POSIX:
>             orig_name = sms._name
>             try:
>                 sms._name = orig_name + '\0a'
>                 with self.assertRaises(ValueError):
>                     sms.unlink()
>             finally:
>                 sms._name = orig_name
> 
4108c4147
<     def test_invalid_shared_memory_cration(self):
---
>     def test_invalid_shared_memory_creation(self):
5612c5651
<                 if rtype == "noop":
---
>                 if rtype in ("noop", "dummy"):
5613a5653
>                     # or tests
5732a5773,5804
>     def _test_resource_tracker_leak_resources(self, cleanup):
>         # We use a separate instance for testing, since the main global
>         # _resource_tracker may be used to watch test infrastructure.
>         from multiprocessing.resource_tracker import ResourceTracker
>         tracker = ResourceTracker()
>         tracker.ensure_running()
>         self.assertTrue(tracker._check_alive())
> 
>         self.assertIsNone(tracker._exitcode)
>         tracker.register('somename', 'dummy')
>         if cleanup:
>             tracker.unregister('somename', 'dummy')
>             expected_exit_code = 0
>         else:
>             expected_exit_code = 1
> 
>         self.assertTrue(tracker._check_alive())
>         self.assertIsNone(tracker._exitcode)
>         tracker._stop()
>         self.assertEqual(tracker._exitcode, expected_exit_code)
> 
>     def test_resource_tracker_exit_code(self):
>         """
>         Test the exit code of the resource tracker.
> 
>         If no leaked resources were found, exit code should be 0, otherwise 1
>         """
>         for cleanup in [True, False]:
>             with self.subTest(cleanup=cleanup):
>                 self._test_resource_tracker_leak_resources(
>                     cleanup=cleanup,
>                 )
# ----------------------------------------------------------------------
diff Python-3.13.0a5/Modules/_multiprocessing/semaphore.c Python-3.13.0a6/Modules/_multiprocessing/semaphore.c
83a84
> @critical_section
95c96
< /*[clinic end generated code: output=f9998f0b6b0b0872 input=e5b45f5cbb775166]*/
---
> /*[clinic end generated code: output=f9998f0b6b0b0872 input=079ca779975f3ad6]*/
174a176
> @critical_section
182c184
< /*[clinic end generated code: output=b22f53ba96b0d1db input=ba7e63a961885d3d]*/
---
> /*[clinic end generated code: output=b22f53ba96b0d1db input=9bd62d3645e7a531]*/
299a302
> @critical_section
311c314
< /*[clinic end generated code: output=f9998f0b6b0b0872 input=e5b45f5cbb775166]*/
---
> /*[clinic end generated code: output=f9998f0b6b0b0872 input=079ca779975f3ad6]*/
384a388
> @critical_section
392c396
< /*[clinic end generated code: output=b22f53ba96b0d1db input=ba7e63a961885d3d]*/
---
> /*[clinic end generated code: output=b22f53ba96b0d1db input=9bd62d3645e7a531]*/
585a590
> @critical_section
593c598
< /*[clinic end generated code: output=5ba8213900e517bb input=36fc59b1cd1025ab]*/
---
> /*[clinic end generated code: output=5ba8213900e517bb input=9fa6e0b321b16935]*/
diff Python-3.13.0a5/Lib/test/_test_multiprocessing.py Python-3.13.0a6/Lib/test/_test_multiprocessing.py 
4665c4665
<             sys.setswitchinterval(1e-6)
---
>             support.setswitchinterval(1e-6)
# ----------------------------------------------------------------------
diff Python-3.13.0a6/Modules/_multiprocessing/multiprocessing.c Python-3.13.0b1/Modules/_multiprocessing/multiprocessing.c
279a280
>     {Py_mod_gil, Py_MOD_GIL_NOT_USED},
diff Python-3.13.0a6/Modules/_multiprocessing/posixshmem.c Python-3.13.0b1/Modules/_multiprocessing/posixshmem.c
5c5
< // Need limited C API version 3.12 for Py_MOD_PER_INTERPRETER_GIL_SUPPORTED
---
> // Need limited C API version 3.13 for Py_mod_gil
8c8
< #  define Py_LIMITED_API 0x030c0000
---
> #  define Py_LIMITED_API 0x030d0000
130a131
>     {Py_mod_gil, Py_MOD_GIL_NOT_USED},
diff Python-3.13.0a6/Lib/multiprocessing/forkserver.py Python-3.13.0b1/Lib/multiprocessing/forkserver.py
0a1
> import atexit
273a275,276
>                                 atexit._clear()
>                                 atexit.register(util._exit_function)
280a284
>                                 atexit._run_exitfuncs()
diff Python-3.13.0a6/Lib/multiprocessing/popen_fork.py Python-3.13.0b1/Lib/multiprocessing/popen_fork.py
0a1
> import atexit
68a70,71
>                 atexit._clear()
>                 atexit.register(util._exit_function)
72a76
>                 atexit._run_exitfuncs()
diff Python-3.13.0a6/Lib/multiprocessing/popen_spawn_win32.py Python-3.13.0b1/Lib/multiprocessing/popen_spawn_win32.py
5a6
> from subprocess import STARTUPINFO, STARTF_FORCEOFFFEEDBACK
77c78,79
<                     None, None, False, 0, env, None, None)
---
>                     None, None, False, 0, env, None,
>                     STARTUPINFO(dwFlags=STARTF_FORCEOFFFEEDBACK))
diff Python-3.13.0a6/Lib/multiprocessing/process.py Python-3.13.0b1/Lib/multiprocessing/process.py
313,317c313,314
<             try:
<                 self.run()
<                 exitcode = 0
<             finally:
<                 util._exit_function()
---
>             self.run()
>             exitcode = 0
diff Python-3.13.0a6/Lib/test/_test_multiprocessing.py Python-3.13.0b1/Lib/test/_test_multiprocessing.py 
2815d2814
<         gc.collect()  # For PyPy or other GCs.
2816a2816
>         support.gc_collect()  # For PyPy or other GCs.
6163a6164,6186
> class _TestAtExit(BaseTestCase):
> 
>     ALLOWED_TYPES = ('processes',)
> 
>     @classmethod
>     def _write_file_at_exit(self, output_path):
>         import atexit
>         def exit_handler():
>             with open(output_path, 'w') as f:
>                 f.write("deadbeef")
>         atexit.register(exit_handler)
> 
>     def test_atexit(self):
>         # gh-83856
>         with os_helper.temp_dir() as temp_dir:
>             output_path = os.path.join(temp_dir, 'output.txt')
>             p = self.Process(target=self._write_file_at_exit, args=(output_path,))
>             p.start()
>             p.join()
>             with open(output_path) as f:
>                 self.assertEqual(f.read(), 'deadbeef')
> 
> 
# ----------------------------------------------------------------------
diff Python-3.13.0b1/Modules/_multiprocessing Python-3.13.0rc1/Modules/_multiprocessing/
diff Python-3.13.0b1/Modules/_multiprocessing/semaphore.c Python-3.13.0rc1/Modules/_multiprocessing/semaphore.c
684a685
> @critical_section
692c693
< /*[clinic end generated code: output=beeb2f07c858511f input=c5e27d594284690b]*/
---
> /*[clinic end generated code: output=beeb2f07c858511f input=d35c9860992ee790]*/
697a699
> @critical_section
712c714
< /*[clinic end generated code: output=3b37c1a9f8b91a03 input=7d644b64a89903f8]*/
---
> /*[clinic end generated code: output=3b37c1a9f8b91a03 input=1610c8cc3e0e337e]*/
# ----------------------------------------------------------------------
diff Python-3.13.0b1/Modules/_multiprocessing/clinic/ Python-3.13.0rc1/Modules/_multiprocessing/clinic/
diff Python-3.13.0b1/Modules/_multiprocessing/clinic/semaphore.c.h Python-3.13.0rc1/Modules/_multiprocessing/clinic/semaphore.c.h
476c476,482
<     return _multiprocessing_SemLock___enter___impl(self);
---
>     PyObject *return_value = NULL;
> 
>     Py_BEGIN_CRITICAL_SECTION(self);
>     return_value = _multiprocessing_SemLock___enter___impl(self);
>     Py_END_CRITICAL_SECTION();
> 
>     return return_value;
520a527
>     Py_BEGIN_CRITICAL_SECTION(self);
521a529
>     Py_END_CRITICAL_SECTION();
568c576
< /*[clinic end generated code: output=713b597256233716 input=a9049054013a1b77]*/
---
> /*[clinic end generated code: output=dea36482d23a355f input=a9049054013a1b77]*/
# ----------------------------------------------------------------------
diff Python-3.13.0b1/Lib/test/_test_multiprocessing.py Python-3.13.0rc1/Lib/test/_test_multiprocessing.py 
25d24
< import pathlib
327,328c326,328
<             sys.executable.encode(),      # bytes
<             pathlib.Path(sys.executable)  # os.PathLike
---
>             os.fsencode(sys.executable),  # bytes
>             os_helper.FakePath(sys.executable),  # os.PathLike
>             os_helper.FakePath(os.fsencode(sys.executable)),  # os.PathLike bytes
1334a1335,1351
>     def test_closed_queue_empty_exceptions(self):
>         # Assert that checking the emptiness of an unused closed queue
>         # does not raise an OSError. The rationale is that q.close() is
>         # a no-op upon construction and becomes effective once the queue
>         # has been used (e.g., by calling q.put()).
>         for q in multiprocessing.Queue(), multiprocessing.JoinableQueue():
>             q.close()  # this is a no-op since the feeder thread is None
>             q.join_thread()  # this is also a no-op
>             self.assertTrue(q.empty())
> 
>         for q in multiprocessing.Queue(), multiprocessing.JoinableQueue():
>             q.put('foo')  # make sure that the queue is 'used'
>             q.close()  # close the feeder thread
>             q.join_thread()  # make sure to join the feeder thread
>             with self.assertRaisesRegex(OSError, 'is closed'):
>                 q.empty()
> 
5817a5835,5843
>     def test_empty_exceptions(self):
>         # Assert that checking emptiness of a closed queue raises
>         # an OSError, independently of whether the queue was used
>         # or not. This differs from Queue and JoinableQueue.
>         q = multiprocessing.SimpleQueue()
>         q.close()  # close the pipe
>         with self.assertRaisesRegex(OSError, 'is closed'):
>             q.empty()
> 
# ----------------------------------------------------------------------
diff Python-3.13.0rc1/Lib/multiprocessing/connection.py Python-3.13.1/Lib/multiprocessing/connection.py
959c959
<         raise AuthenticationError('challenge too short: {len(message)} bytes')
---
>         raise AuthenticationError(f'challenge too short: {len(message)} bytes')
diff Python-3.13.0rc1/Lib/multiprocessing/forkserver.py Python-3.13.1/Lib/multiprocessing/forkserver.py
170a171,172
>         if sys_path is not None:
>             sys.path[:] = sys_path
diff Python-3.13.0rc1/Lib/multiprocessing/managers.py Python-3.13.1/Lib/multiprocessing/managers.py
760a761,764
>     # Each instance gets a `_serial` number. Unlike `id(...)`, this number
>     # is never reused.
>     _next_serial = 1
> 
764,767c768,774
<             tls_idset = BaseProxy._address_to_local.get(token.address, None)
<             if tls_idset is None:
<                 tls_idset = util.ForkAwareLocal(), ProcessLocalSet()
<                 BaseProxy._address_to_local[token.address] = tls_idset
---
>             tls_serials = BaseProxy._address_to_local.get(token.address, None)
>             if tls_serials is None:
>                 tls_serials = util.ForkAwareLocal(), ProcessLocalSet()
>                 BaseProxy._address_to_local[token.address] = tls_serials
> 
>             self._serial = BaseProxy._next_serial
>             BaseProxy._next_serial += 1
771c778
<         self._tls = tls_idset[0]
---
>         self._tls = tls_serials[0]
773,774c780,781
<         # self._idset is used to record the identities of all shared
<         # objects for which the current process owns references and
---
>         # self._all_serials is a set used to record the identities of all
>         # shared objects for which the current process owns references and
776c783
<         self._idset = tls_idset[1]
---
>         self._all_serials = tls_serials[1]
859c866
<         self._idset.add(self._id)
---
>         self._all_serials.add(self._serial)
865,866c872,873
<             args=(self._token, self._authkey, state,
<                   self._tls, self._idset, self._Client),
---
>             args=(self._token, self._serial, self._authkey, state,
>                   self._tls, self._all_serials, self._Client),
871,872c878,879
<     def _decref(token, authkey, state, tls, idset, _Client):
<         idset.discard(token.id)
---
>     def _decref(token, serial, authkey, state, tls, idset, _Client):
>         idset.discard(serial)
diff Python-3.13.0rc1/Lib/multiprocessing/synchronize.py Python-3.13.1/Lib/multiprocessing/synchronize.py
177c177
<             elif self._semlock._get_value() == 1:
---
>             elif not self._semlock._is_zero():
203c203
<             elif self._semlock._get_value() == 1:
---
>             elif not self._semlock._is_zero():
diff Python-3.13.0rc1/Lib/test/_test_multiprocessing.py Python-3.13.1/Lib/test/_test_multiprocessing.py 
14a15
> import importlib
21a23
> import shutil
23a26
> import tempfile
256a260,262
>     # If not empty, limit which start method suites run this class.
>     START_METHODS: set[str] = set()
>     start_method = None  # set by install_tests_in_module_dict()
1364a1371,1430
>     @staticmethod
>     def _acquire(lock, l=None):
>         lock.acquire()
>         if l is not None:
>             l.append(repr(lock))
> 
>     @staticmethod
>     def _acquire_event(lock, event):
>         lock.acquire()
>         event.set()
>         time.sleep(1.0)
> 
>     def test_repr_lock(self):
>         if self.TYPE != 'processes':
>             self.skipTest('test not appropriate for {}'.format(self.TYPE))
> 
>         lock = self.Lock()
>         self.assertEqual(f'<Lock(owner=None)>', repr(lock))
> 
>         lock.acquire()
>         self.assertEqual(f'<Lock(owner=MainProcess)>', repr(lock))
>         lock.release()
> 
>         tname = 'T1'
>         l = []
>         t = threading.Thread(target=self._acquire,
>                              args=(lock, l),
>                              name=tname)
>         t.start()
>         time.sleep(0.1)
>         self.assertEqual(f'<Lock(owner=MainProcess|{tname})>', l[0])
>         lock.release()
> 
>         t = threading.Thread(target=self._acquire,
>                              args=(lock,),
>                              name=tname)
>         t.start()
>         time.sleep(0.1)
>         self.assertEqual('<Lock(owner=SomeOtherThread)>', repr(lock))
>         lock.release()
> 
>         pname = 'P1'
>         l = multiprocessing.Manager().list()
>         p = self.Process(target=self._acquire,
>                          args=(lock, l),
>                          name=pname)
>         p.start()
>         p.join()
>         self.assertEqual(f'<Lock(owner={pname})>', l[0])
> 
>         lock = self.Lock()
>         event = self.Event()
>         p = self.Process(target=self._acquire_event,
>                          args=(lock, event),
>                          name='P2')
>         p.start()
>         event.wait()
>         self.assertEqual(f'<Lock(owner=SomeOtherProcess)>', repr(lock))
>         p.terminate()
> 
1371a1438,1499
>     @staticmethod
>     def _acquire_release(lock, timeout, l=None, n=1):
>         for _ in range(n):
>             lock.acquire()
>         if l is not None:
>             l.append(repr(lock))
>         time.sleep(timeout)
>         for _ in range(n):
>             lock.release()
> 
>     def test_repr_rlock(self):
>         if self.TYPE != 'processes':
>             self.skipTest('test not appropriate for {}'.format(self.TYPE))
> 
>         lock = self.RLock()
>         self.assertEqual('<RLock(None, 0)>', repr(lock))
> 
>         n = 3
>         for _ in range(n):
>             lock.acquire()
>         self.assertEqual(f'<RLock(MainProcess, {n})>', repr(lock))
>         for _ in range(n):
>             lock.release()
> 
>         t, l = [], []
>         for i in range(n):
>             t.append(threading.Thread(target=self._acquire_release,
>                                       args=(lock, 0.1, l, i+1),
>                                       name=f'T{i+1}'))
>             t[-1].start()
>         for t_ in t:
>             t_.join()
>         for i in range(n):
>             self.assertIn(f'<RLock(MainProcess|T{i+1}, {i+1})>', l)
> 
> 
>         t = threading.Thread(target=self._acquire_release,
>                                  args=(lock, 0.2),
>                                  name=f'T1')
>         t.start()
>         time.sleep(0.1)
>         self.assertEqual('<RLock(SomeOtherThread, nonzero)>', repr(lock))
>         time.sleep(0.2)
> 
>         pname = 'P1'
>         l = multiprocessing.Manager().list()
>         p = self.Process(target=self._acquire_release,
>                          args=(lock, 0.1, l),
>                          name=pname)
>         p.start()
>         p.join()
>         self.assertEqual(f'<RLock({pname}, 1)>', l[0])
> 
>         event = self.Event()
>         lock = self.RLock()
>         p = self.Process(target=self._acquire_event,
>                          args=(lock, event))
>         p.start()
>         event.wait()
>         self.assertEqual('<RLock(SomeOtherProcess, nonzero)>', repr(lock))
>         p.join()
> 
5749a5878,5879
>     @unittest.skipIf(sys.platform.startswith("netbsd"),
>                      "gh-125620: Skip on NetBSD due to long wait for SIGKILL process termination.")
6212a6343,6412
> class _TestSpawnedSysPath(BaseTestCase):
>     """Test that sys.path is setup in forkserver and spawn processes."""
> 
>     ALLOWED_TYPES = {'processes'}
>     # Not applicable to fork which inherits everything from the process as is.
>     START_METHODS = {"forkserver", "spawn"}
> 
>     def setUp(self):
>         self._orig_sys_path = list(sys.path)
>         self._temp_dir = tempfile.mkdtemp(prefix="test_sys_path-")
>         self._mod_name = "unique_test_mod"
>         module_path = os.path.join(self._temp_dir, f"{self._mod_name}.py")
>         with open(module_path, "w", encoding="utf-8") as mod:
>             mod.write("# A simple test module\n")
>         sys.path[:] = [p for p in sys.path if p]  # remove any existing ""s
>         sys.path.insert(0, self._temp_dir)
>         sys.path.insert(0, "")  # Replaced with an abspath in child.
>         self.assertIn(self.start_method, self.START_METHODS)
>         self._ctx = multiprocessing.get_context(self.start_method)
> 
>     def tearDown(self):
>         sys.path[:] = self._orig_sys_path
>         shutil.rmtree(self._temp_dir, ignore_errors=True)
> 
>     @staticmethod
>     def enq_imported_module_names(queue):
>         queue.put(tuple(sys.modules))
> 
>     def test_forkserver_preload_imports_sys_path(self):
>         if self._ctx.get_start_method() != "forkserver":
>             self.skipTest("forkserver specific test.")
>         self.assertNotIn(self._mod_name, sys.modules)
>         multiprocessing.forkserver._forkserver._stop()  # Must be fresh.
>         self._ctx.set_forkserver_preload(
>             ["test.test_multiprocessing_forkserver", self._mod_name])
>         q = self._ctx.Queue()
>         proc = self._ctx.Process(
>                 target=self.enq_imported_module_names, args=(q,))
>         proc.start()
>         proc.join()
>         child_imported_modules = q.get()
>         q.close()
>         self.assertIn(self._mod_name, child_imported_modules)
> 
>     @staticmethod
>     def enq_sys_path_and_import(queue, mod_name):
>         queue.put(sys.path)
>         try:
>             importlib.import_module(mod_name)
>         except ImportError as exc:
>             queue.put(exc)
>         else:
>             queue.put(None)
> 
>     def test_child_sys_path(self):
>         q = self._ctx.Queue()
>         proc = self._ctx.Process(
>                 target=self.enq_sys_path_and_import, args=(q, self._mod_name))
>         proc.start()
>         proc.join()
>         child_sys_path = q.get()
>         import_error = q.get()
>         q.close()
>         self.assertNotIn("", child_sys_path)  # replaced by an abspath
>         self.assertIn(self._temp_dir, child_sys_path)  # our addition
>         # ignore the first element, it is the absolute "" replacement
>         self.assertEqual(child_sys_path[1:], sys.path[1:])
>         self.assertIsNone(import_error, msg=f"child could not import {self._mod_name}")
> 
> 
6406a6607,6608
>             if base.START_METHODS and start_method not in base.START_METHODS:
>                 continue  # class not intended for this start method.
6419a6622
>                 Temp.start_method = start_method
# ----------------------------------------------------------------------
diff Python-3.13.1/Modules/_multiprocessing/semaphore.c Python-3.13.2/Modules/_multiprocessing/semaphore.c
29a30,31
> #define _SemLockObject_CAST(op) ((SemLockObject *)(op))
> 
578c580
< semlock_dealloc(SemLockObject* self)
---
> semlock_dealloc(PyObject *op)
579a582
>     SemLockObject *self = _SemLockObject_CAST(op);
720c723
< semlock_traverse(SemLockObject *s, visitproc visit, void *arg)
---
> semlock_traverse(PyObject *s, visitproc visit, void *arg)
diff Python-3.13.1/Lib/multiprocessing/connection.py Python-3.13.2/Lib/multiprocessing/connection.py
849c849
< def _get_digest_name_and_payload(message: bytes) -> (str, bytes):
---
> def _get_digest_name_and_payload(message):  # type: (bytes) -> tuple[str, bytes]
diff Python-3.13.1/Lib/multiprocessing/resource_tracker.py Python-3.13.2/Lib/multiprocessing/resource_tracker.py
157a158
>                 prev_sigmask = None
160c161
<                         signal.pthread_sigmask(signal.SIG_BLOCK, _IGNORED_SIGNALS)
---
>                         prev_sigmask = signal.pthread_sigmask(signal.SIG_BLOCK, _IGNORED_SIGNALS)
163,164c164,165
<                     if _HAVE_SIGMASK:
<                         signal.pthread_sigmask(signal.SIG_UNBLOCK, _IGNORED_SIGNALS)
---
>                     if prev_sigmask is not None:
>                         signal.pthread_sigmask(signal.SIG_SETMASK, prev_sigmask)
diff Python-3.13.1/Lib/multiprocessing/synchronize.py Python-3.13.2/Lib/multiprocessing/synchronize.py
363c363
<     def __repr__(self) -> str:
---
>     def __repr__(self):
diff Python-3.13.1/Lib/test/_test_multiprocessing.py Python-3.13.2/Lib/test/_test_multiprocessing.py 
5952a5953,5973
>     @unittest.skipUnless(hasattr(signal, "pthread_sigmask"), "pthread_sigmask is not available")
>     def test_resource_tracker_blocked_signals(self):
>         #
>         # gh-127586: Check that resource_tracker does not override blocked signals of caller.
>         #
>         from multiprocessing.resource_tracker import ResourceTracker
>         orig_sigmask = signal.pthread_sigmask(signal.SIG_BLOCK, set())
>         signals = {signal.SIGTERM, signal.SIGINT, signal.SIGUSR1}
> 
>         try:
>             for sig in signals:
>                 signal.pthread_sigmask(signal.SIG_SETMASK, {sig})
>                 self.assertEqual(signal.pthread_sigmask(signal.SIG_BLOCK, set()), {sig})
>                 tracker = ResourceTracker()
>                 tracker.ensure_running()
>                 self.assertEqual(signal.pthread_sigmask(signal.SIG_BLOCK, set()), {sig})
>                 tracker._stop()
>         finally:
>             # restore sigmask to what it was before executing test
>             signal.pthread_sigmask(signal.SIG_SETMASK, orig_sigmask)
> 
# ----------------------------------------------------------------------
diff Python-3.13.2/Lib/multiprocessing/resource_tracker.py Python-3.13.3/Lib/multiprocessing/resource_tracker.py
78,86c78,94
<     def _stop(self):
<         with self._lock:
<             # This should not happen (_stop() isn't called by a finalizer)
<             # but we check for it anyway.
<             if self._lock._recursion_count() > 1:
<                 return self._reentrant_call_error()
<             if self._fd is None:
<                 # not running
<                 return
---
>     def __del__(self):
>         # making sure child processess are cleaned before ResourceTracker
>         # gets destructed.
>         # see https://github.com/python/cpython/issues/88887
>         self._stop(use_blocking_lock=False)
> 
>     def _stop(self, use_blocking_lock=True):
>         if use_blocking_lock:
>             with self._lock:
>                 self._stop_locked()
>         else:
>             acquired = self._lock.acquire(blocking=False)
>             try:
>                 self._stop_locked()
>             finally:
>                 if acquired:
>                     self._lock.release()
88,90c96,114
<             # closing the "alive" file descriptor stops main()
<             os.close(self._fd)
<             self._fd = None
---
>     def _stop_locked(
>         self,
>         close=os.close,
>         waitpid=os.waitpid,
>         waitstatus_to_exitcode=os.waitstatus_to_exitcode,
>     ):
>         # This shouldn't happen (it might when called by a finalizer)
>         # so we check for it anyway.
>         if self._lock._recursion_count() > 1:
>             return self._reentrant_call_error()
>         if self._fd is None:
>             # not running
>             return
>         if self._pid is None:
>             return
> 
>         # closing the "alive" file descriptor stops main()
>         close(self._fd)
>         self._fd = None
92c116
<             _, status = os.waitpid(self._pid, 0)
---
>         _, status = waitpid(self._pid, 0)
94c118
<             self._pid = None
---
>         self._pid = None
96,100c120,124
<             try:
<                 self._exitcode = os.waitstatus_to_exitcode(status)
<             except ValueError:
<                 # os.waitstatus_to_exitcode may raise an exception for invalid values
<                 self._exitcode = None
---
>         try:
>             self._exitcode = waitstatus_to_exitcode(status)
>         except ValueError:
>             # os.waitstatus_to_exitcode may raise an exception for invalid values
>             self._exitcode = None
diff Python-3.13.2/Lib/test/_test_multiprocessing.py Python-3.13.3/Lib/test/_test_multiprocessing.py 
591c591,592
<         p = self.Process(target=time.sleep, args=(DELTA,))
---
>         event = self.Event()
>         p = self.Process(target=event.wait, args=())
594,596c595,600
<         p.daemon = True
<         p.start()
<         self.assertIn(p, self.active_children())
---
>         try:
>             p.daemon = True
>             p.start()
>             self.assertIn(p, self.active_children())
>         finally:
>             event.set()
1580c1584
<         for i in range(10):
---
>         for _ in support.sleeping_retry(support.SHORT_TIMEOUT):
1586,1587c1590
<             time.sleep(DELTA)
<         time.sleep(DELTA)
---
> 
1609d1611
<         self.addCleanup(p.join)
1611,1614c1613,1615
<         p = threading.Thread(target=self.f, args=(cond, sleeping, woken))
<         p.daemon = True
<         p.start()
<         self.addCleanup(p.join)
---
>         t = threading.Thread(target=self.f, args=(cond, sleeping, woken))
>         t.daemon = True
>         t.start()
1621,1622c1622
<         time.sleep(DELTA)
<         self.assertReturnsIfImplemented(0, get_value, woken)
---
>         self.assertReachesEventually(lambda: get_value(woken), 0)
1630,1631c1630
<         time.sleep(DELTA)
<         self.assertReturnsIfImplemented(1, get_value, woken)
---
>         self.assertReachesEventually(lambda: get_value(woken), 1)
1639,1640c1638
<         time.sleep(DELTA)
<         self.assertReturnsIfImplemented(2, get_value, woken)
---
>         self.assertReachesEventually(lambda: get_value(woken), 2)
1644c1642,1644
<         p.join()
---
> 
>         threading_helper.join_thread(t)
>         join_process(p)
1651a1652
>         workers = []
1657c1658
<             self.addCleanup(p.join)
---
>             workers.append(p)
1663c1664
<             self.addCleanup(t.join)
---
>             workers.append(t)
1682c1683
<             self.addCleanup(p.join)
---
>             workers.append(p)
1687c1688
<             self.addCleanup(t.join)
---
>             workers.append(t)
1703c1704,1706
<         self.assertReachesEventually(lambda: get_value(woken), 6)
---
>         for i in range(6):
>             woken.acquire()
>         self.assertReturnsIfImplemented(0, get_value, woken)
1707a1711,1714
>         for w in workers:
>             # NOTE: join_process and join_thread are the same
>             threading_helper.join_thread(w)
> 
1713a1721
>         workers = []
1718c1726
<             self.addCleanup(p.join)
---
>             workers.append(p)
1723c1731
<             self.addCleanup(t.join)
---
>             workers.append(t)
1757a1766,1769
>         for w in workers:
>             # NOTE: join_process and join_thread are the same
>             threading_helper.join_thread(w)
> 
# ----------------------------------------------------------------------
diff Python-3.13.3/Lib/multiprocessing/context.py Python-3.13.4/Lib/multiprocessing/context.py
148c148
<         if sys.platform == 'win32' and getattr(sys, 'frozen', False):
---
>         if self.get_start_method() == 'spawn' and getattr(sys, 'frozen', False):
diff Python-3.13.3/Lib/test/_test_multiprocessing.py Python-3.13.4/Lib/test/_test_multiprocessing.py 
514a515,519
>     def _sleep_some_event(cls, event):
>         event.set()
>         time.sleep(100)
> 
>     @classmethod
522c527,528
<         p = self.Process(target=self._sleep_some)
---
>         event = self.Event()
>         p = self.Process(target=self._sleep_some_event, args=(event,))
540,541c546,550
<         # XXX maybe terminating too soon causes the problems on Gentoo...
<         time.sleep(1)
---
>         timeout = support.SHORT_TIMEOUT
>         if not event.wait(timeout):
>             p.terminate()
>             p.join()
>             self.fail(f"event not signaled in {timeout} seconds")
6487a6497,6518
>     def test_forked_thread_not_started(self):
>         # gh-134381: Ensure that a thread that has not been started yet in
>         # the parent process can be started within a forked child process.
> 
>         if multiprocessing.get_start_method() != "fork":
>             self.skipTest("fork specific test")
> 
>         q = multiprocessing.Queue()
>         t = threading.Thread(target=lambda: q.put("done"), daemon=True)
> 
>         def child():
>             t.start()
>             t.join()
> 
>         p = multiprocessing.Process(target=child)
>         p.start()
>         p.join(support.SHORT_TIMEOUT)
> 
>         self.assertEqual(p.exitcode, 0)
>         self.assertEqual(q.get_nowait(), "done")
>         close_queue(q)
> 
# ----------------------------------------------------------------------
diff Python-3.13.4/Lib/multiprocessing/connection.py Python-3.13.6/Lib/multiprocessing/connection.py
77c77
<         return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())
---
>         return tempfile.mktemp(prefix='sock-', dir=util.get_temp_dir())
diff Python-3.13.4/Lib/multiprocessing/forkserver.py Python-3.13.6/Lib/multiprocessing/forkserver.py
184a185,188
>         # gh-135335: flush stdout/stderr in case any of the preloaded modules
>         # wrote to them, otherwise children might inherit buffered data
>         util._flush_std_streams()
> 
diff Python-3.13.4/Lib/multiprocessing/util.py Python-3.13.6/Lib/multiprocessing/util.py
36a37
> WARNING = 30
55a57,60
> def _warn(msg, *args):
>     if _logger:
>         _logger.log(WARNING, msg, *args, stacklevel=2)
> 
123a129,143
> # Maximum length of a socket file path is usually between 92 and 108 [1],
> # but Linux is known to use a size of 108 [2]. BSD-based systems usually
> # use a size of 104 or 108 and Windows does not create AF_UNIX sockets.
> #
> # [1]: https://pubs.opengroup.org/onlinepubs/9799919799/basedefs/sys_un.h.html
> # [2]: https://man7.org/linux/man-pages/man7/unix.7.html.
> 
> if sys.platform == 'linux':
>     _SUN_PATH_MAX = 108
> elif sys.platform.startswith(('openbsd', 'freebsd')):
>     _SUN_PATH_MAX = 104
> else:
>     # On Windows platforms, we do not create AF_UNIX sockets.
>     _SUN_PATH_MAX = None if os.name == 'nt' else 92
> 
132a153,206
> def _get_base_temp_dir(tempfile):
>     """Get a temporary directory where socket files will be created.
> 
>     To prevent additional imports, pass a pre-imported 'tempfile' module.
>     """
>     if os.name == 'nt':
>         return None
>     # Most of the time, the default temporary directory is /tmp. Thus,
>     # listener sockets files "$TMPDIR/pymp-XXXXXXXX/sock-XXXXXXXX" do
>     # not have a path length exceeding SUN_PATH_MAX.
>     #
>     # If users specify their own temporary directory, we may be unable
>     # to create those files. Therefore, we fall back to the system-wide
>     # temporary directory /tmp, assumed to exist on POSIX systems.
>     #
>     # See https://github.com/python/cpython/issues/132124.
>     base_tempdir = tempfile.gettempdir()
>     # Files created in a temporary directory are suffixed by a string
>     # generated by tempfile._RandomNameSequence, which, by design,
>     # is 8 characters long.
>     #
>     # Thus, the length of socket filename will be:
>     #
>     #   len(base_tempdir + '/pymp-XXXXXXXX' + '/sock-XXXXXXXX')
>     sun_path_len = len(base_tempdir) + 14 + 14
>     if sun_path_len <= _SUN_PATH_MAX:
>         return base_tempdir
>     # Fallback to the default system-wide temporary directory.
>     # This ignores user-defined environment variables.
>     #
>     # On POSIX systems, /tmp MUST be writable by any application [1].
>     # We however emit a warning if this is not the case to prevent
>     # obscure errors later in the execution.
>     #
>     # On some legacy systems, /var/tmp and /usr/tmp can be present
>     # and will be used instead.
>     #
>     # [1]: https://refspecs.linuxfoundation.org/FHS_3.0/fhs/ch03s18.html
>     dirlist = ['/tmp', '/var/tmp', '/usr/tmp']
>     try:
>         base_system_tempdir = tempfile._get_default_tempdir(dirlist)
>     except FileNotFoundError:
>         _warn("Process-wide temporary directory %s will not be usable for "
>               "creating socket files and no usable system-wide temporary "
>               "directory was found in %s", base_tempdir, dirlist)
>         # At this point, the system-wide temporary directory is not usable
>         # but we may assume that the user-defined one is, even if we will
>         # not be able to write socket files out there.
>         return base_tempdir
>     _warn("Ignoring user-defined temporary directory: %s", base_tempdir)
>     # at most max(map(len, dirlist)) + 14 + 14 = 36 characters
>     assert len(base_system_tempdir) + 14 + 14 <= _SUN_PATH_MAX
>     return base_system_tempdir
> 
138c212,213
<         tempdir = tempfile.mkdtemp(prefix='pymp-')
---
>         base_tempdir = _get_base_temp_dir(tempfile)
>         tempdir = tempfile.mkdtemp(prefix='pymp-', dir=base_tempdir)
diff Python-3.13.4/Lib/test/_test_multiprocessing.py Python-3.13.6/Lib/test/_test_multiprocessing.py 
6453a6454,6482
>     def test_std_streams_flushed_after_preload(self):
>         # gh-135335: Check fork server flushes standard streams after
>         # preloading modules
>         if multiprocessing.get_start_method() != "forkserver":
>             self.skipTest("forkserver specific test")
> 
>         # Create a test module in the temporary directory on the child's path
>         # TODO: This can all be simplified once gh-126631 is fixed and we can
>         #       use __main__ instead of a module.
>         dirname = os.path.join(self._temp_dir, 'preloaded_module')
>         init_name = os.path.join(dirname, '__init__.py')
>         os.mkdir(dirname)
>         with open(init_name, "w") as f:
>             cmd = '''if 1:
>                 import sys
>                 print('stderr', end='', file=sys.stderr)
>                 print('stdout', end='', file=sys.stdout)
>             '''
>             f.write(cmd)
> 
>         name = os.path.join(os.path.dirname(__file__), 'mp_preload_flush.py')
>         env = {'PYTHONPATH': self._temp_dir}
>         _, out, err = test.support.script_helper.assert_python_ok(name, **env)
> 
>         # Check stderr first, as it is more likely to be useful to see in the
>         # event of a failure.
>         self.assertEqual(err.decode().rstrip(), 'stderr')
>         self.assertEqual(out.decode().rstrip(), 'stdout')
> 
# ----------------------------------------------------------------------
diff Python-3.13.6/Lib/multiprocessing/resource_tracker.py Python-3.13.7/Lib/multiprocessing/resource_tracker.py
22a23
> from collections import deque
68a70
>         self._reentrant_messages = deque()
105c107
<             return self._reentrant_call_error()
---
>             raise self._reentrant_call_error()
134a137,198
>         return self._ensure_running_and_write()
> 
>     def _teardown_dead_process(self):
>         os.close(self._fd)
> 
>         # Clean-up to avoid dangling processes.
>         try:
>             # _pid can be None if this process is a child from another
>             # python process, which has started the resource_tracker.
>             if self._pid is not None:
>                 os.waitpid(self._pid, 0)
>         except ChildProcessError:
>             # The resource_tracker has already been terminated.
>             pass
>         self._fd = None
>         self._pid = None
>         self._exitcode = None
> 
>         warnings.warn('resource_tracker: process died unexpectedly, '
>                       'relaunching.  Some resources might leak.')
> 
>     def _launch(self):
>         fds_to_pass = []
>         try:
>             fds_to_pass.append(sys.stderr.fileno())
>         except Exception:
>             pass
>         r, w = os.pipe()
>         try:
>             fds_to_pass.append(r)
>             # process will out live us, so no need to wait on pid
>             exe = spawn.get_executable()
>             args = [
>                 exe,
>                 *util._args_from_interpreter_flags(),
>                 '-c',
>                 f'from multiprocessing.resource_tracker import main;main({r})',
>             ]
>             # bpo-33613: Register a signal mask that will block the signals.
>             # This signal mask will be inherited by the child that is going
>             # to be spawned and will protect the child from a race condition
>             # that can make the child die before it registers signal handlers
>             # for SIGINT and SIGTERM. The mask is unregistered after spawning
>             # the child.
>             prev_sigmask = None
>             try:
>                 if _HAVE_SIGMASK:
>                     prev_sigmask = signal.pthread_sigmask(signal.SIG_BLOCK, _IGNORED_SIGNALS)
>                 pid = util.spawnv_passfds(exe, args, fds_to_pass)
>             finally:
>                 if prev_sigmask is not None:
>                     signal.pthread_sigmask(signal.SIG_SETMASK, prev_sigmask)
>         except:
>             os.close(w)
>             raise
>         else:
>             self._fd = w
>             self._pid = pid
>         finally:
>             os.close(r)
> 
>     def _ensure_running_and_write(self, msg=None):
138c202,205
<                 return self._reentrant_call_error()
---
>                 if msg is None:
>                     raise self._reentrant_call_error()
>                 return self._reentrant_messages.append(msg)
> 
141,147c208,211
<                 if self._check_alive():
<                     # => still alive
<                     return
<                 # => dead, launch it again
<                 os.close(self._fd)
< 
<                 # Clean-up to avoid dangling processes.
---
>                 if msg is None:
>                     to_send = b'PROBE:0:noop\n'
>                 else:
>                     to_send = msg
149,158c213,216
<                     # _pid can be None if this process is a child from another
<                     # python process, which has started the resource_tracker.
<                     if self._pid is not None:
<                         os.waitpid(self._pid, 0)
<                 except ChildProcessError:
<                     # The resource_tracker has already been terminated.
<                     pass
<                 self._fd = None
<                 self._pid = None
<                 self._exitcode = None
---
>                     self._write(to_send)
>                 except OSError:
>                     self._teardown_dead_process()
>                     self._launch()
160,161c218,220
<                 warnings.warn('resource_tracker: process died unexpectedly, '
<                               'relaunching.  Some resources might leak.')
---
>                 msg = None  # message was sent in probe
>             else:
>                 self._launch()
163c222
<             fds_to_pass = []
---
>         while True:
165,197c224,229
<                 fds_to_pass.append(sys.stderr.fileno())
<             except Exception:
<                 pass
<             cmd = 'from multiprocessing.resource_tracker import main;main(%d)'
<             r, w = os.pipe()
<             try:
<                 fds_to_pass.append(r)
<                 # process will out live us, so no need to wait on pid
<                 exe = spawn.get_executable()
<                 args = [exe] + util._args_from_interpreter_flags()
<                 args += ['-c', cmd % r]
<                 # bpo-33613: Register a signal mask that will block the signals.
<                 # This signal mask will be inherited by the child that is going
<                 # to be spawned and will protect the child from a race condition
<                 # that can make the child die before it registers signal handlers
<                 # for SIGINT and SIGTERM. The mask is unregistered after spawning
<                 # the child.
<                 prev_sigmask = None
<                 try:
<                     if _HAVE_SIGMASK:
<                         prev_sigmask = signal.pthread_sigmask(signal.SIG_BLOCK, _IGNORED_SIGNALS)
<                     pid = util.spawnv_passfds(exe, args, fds_to_pass)
<                 finally:
<                     if prev_sigmask is not None:
<                         signal.pthread_sigmask(signal.SIG_SETMASK, prev_sigmask)
<             except:
<                 os.close(w)
<                 raise
<             else:
<                 self._fd = w
<                 self._pid = pid
<             finally:
<                 os.close(r)
---
>                 reentrant_msg = self._reentrant_messages.popleft()
>             except IndexError:
>                 break
>             self._write(reentrant_msg)
>         if msg is not None:
>             self._write(msg)
217a250,253
>     def _write(self, msg):
>         nbytes = os.write(self._fd, msg)
>         assert nbytes == len(msg), f"{nbytes=} != {len(msg)=}"
> 
219,230c255
<         try:
<             self.ensure_running()
<         except ReentrantCallError:
<             # The code below might or might not work, depending on whether
<             # the resource tracker was already running and still alive.
<             # Better warn the user.
<             # (XXX is warnings.warn itself reentrant-safe? :-)
<             warnings.warn(
<                 f"ResourceTracker called reentrantly for resource cleanup, "
<                 f"which is unsupported. "
<                 f"The {rtype} object {name!r} might leak.")
<         msg = '{0}:{1}:{2}\n'.format(cmd, name, rtype).encode('ascii')
---
>         msg = f"{cmd}:{name}:{rtype}\n".encode("ascii")
235,237d259
<         nbytes = os.write(self._fd, msg)
<         assert nbytes == len(msg), "nbytes {0:n} but len(msg) {1:n}".format(
<             nbytes, len(msg))
238a261
>         self._ensure_running_and_write(msg)
# ----------------------------------------------------------------------
diff Python-3.13.7/Lib/multiprocessing/forkserver.py Python-3.13.8/Lib/multiprocessing/forkserver.py
129a130
>             main_kws = {}
131d131
<                 desired_keys = {'main_path', 'sys_path'}
133,135c133,136
<                 data = {x: y for x, y in data.items() if x in desired_keys}
<             else:
<                 data = {}
---
>                 if 'sys_path' in data:
>                     main_kws['sys_path'] = data['sys_path']
>                 if 'init_main_from_path' in data:
>                     main_kws['main_path'] = data['init_main_from_path']
150c151
<                             data)
---
>                             main_kws)
diff Python-3.13.7/Lib/multiprocessing/popen_spawn_posix.py Python-3.13.8/Lib/multiprocessing/popen_spawn_posix.py
59a60,63
>             os.close(child_r)
>             child_r = None
>             os.close(child_w)
>             child_w = None
diff Python-3.13.7/Lib/test/_test_multiprocessing.py Python-3.13.8/Lib/test/_test_multiprocessing.py 
6547a6548,6559
>     def test_preload_main(self):
>         # gh-126631: Check that __main__ can be pre-loaded
>         if multiprocessing.get_start_method() != "forkserver":
>             self.skipTest("forkserver specific test")
> 
>         name = os.path.join(os.path.dirname(__file__), 'mp_preload_main.py')
>         _, out, err = test.support.script_helper.assert_python_ok(name)
>         self.assertEqual(err, b'')
> 
>         # The trailing empty string comes from split() on output ending with \n
>         out = out.decode().split("\n")
>         self.assertEqual(out, ['__main__', '__mp_main__', 'f', 'f', ''])
